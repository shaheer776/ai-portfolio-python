{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "aa2d46ec",
      "metadata": {
        "id": "aa2d46ec",
        "papermill": {
          "duration": 0.001961,
          "end_time": "2025-06-30T07:42:40.343505",
          "exception": false,
          "start_time": "2025-06-30T07:42:40.341544",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "#  Steps to extract features from your dataset\n",
        "---\n",
        "\n",
        "Here are a few steps to do a basic preprocessing of a dataset. The example dataset used is the `IMDB Dataset of 50K Movie Reviews dataset`\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 📑 Contents\n",
        "\n",
        "1. Lower Casing\n",
        "2. Remove HTML tags\n",
        "3. Remove URLs\n",
        "4. Remove Punctuation\n",
        "5. Chat word treatment\n",
        "6. Spelling Correction\n",
        "7. Removing Stop words\n",
        "8. Handling Emojis\n",
        "9. Tokenization\n",
        "10. Stemming\n",
        "11. Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b98dd653",
      "metadata": {
        "id": "b98dd653"
      },
      "source": [
        "# 1. One Hot Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d2bff62",
      "metadata": {
        "id": "6d2bff62"
      },
      "source": [
        "One-Hot Encoding is a method to convert categorical (textual or class) data into a binary (0 or 1) format that is more suitable for machine learning models.\n",
        "\n",
        "Each category is represented as a binary vector with a 1 indicating the presence of a specific class and 0s elsewhere.\n",
        "Many machine learning algorithms cannot handle categorical values directly (like ['red', 'green', 'blue']) because they expect numerical input. One-hot encoding\n",
        "\n",
        "converts these into binary vectors so the model can process them.\n",
        "\n",
        "For example:\n",
        "\n",
        "### Original data:\n",
        "\n",
        "['red', 'green', 'blue']\n",
        "\n",
        "### After One-Hot Encoding:\n",
        "\n",
        "red   → [1, 0, 0]  \n",
        "green → [0, 1, 0]  \n",
        "blue  → [0, 0, 1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b81c96e",
      "metadata": {
        "id": "5b81c96e"
      },
      "source": [
        "### Pros:\n",
        "\n",
        "| Advantage                                | Description                                                                             |\n",
        "| ---------------------------------------- | --------------------------------------------------------------------------------------- |\n",
        "|  **Simple to implement**               | Very easy and straightforward to apply using libraries like pandas or sklearn.          |\n",
        "|  **No ordinal relationship**           | Prevents the model from assuming a natural ordering (e.g., 'Red' ≠ '1', 'Green' ≠ '2'). |\n",
        "|  **Works well with tree-based models** | Such as Decision Trees, Random Forest, and XGBoost.                                     |\n",
        "\n",
        "\n",
        "### Cons:\n",
        "\n",
        "| Disadvantage                         | Description                                                                           |\n",
        "| ------------------------------------ | ------------------------------------------------------------------------------------- |\n",
        "|  **Curse of dimensionality**       | If there are many categories, the feature space becomes very large and sparse.        |\n",
        "|  **Memory inefficiency**           | Each category creates a new column, consuming memory and slowing training.            |\n",
        "|  **Sparcity** | Sparse vectors can be inefficient for deep learning (embeddings are often preferred). |\n",
        "|  **Out of vocabulary**             | Can not represent out of vocabulary input words.                                      |\n",
        "|  **No Fixed Size**                 | Only fixed size of input can be given                                                 |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c95679bf",
      "metadata": {
        "id": "c95679bf",
        "outputId": "0ce059f8-f947-4814-ad82-6d25be1a4598"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Color_Blue  Color_Green  Color_Red\n",
            "0       False        False       True\n",
            "1       False         True      False\n",
            "2        True        False      False\n",
            "3       False        False       True\n"
          ]
        }
      ],
      "source": [
        "# Example in Python using pandas:\n",
        "import pandas as pd\n",
        "\n",
        "# Sample data\n",
        "df = pd.DataFrame({'Color': ['Red', 'Green', 'Blue', 'Red']})\n",
        "\n",
        "# One-hot encoding\n",
        "encoded = pd.get_dummies(df, columns=['Color'])\n",
        "\n",
        "print(encoded)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39d2e699",
      "metadata": {
        "id": "39d2e699"
      },
      "source": [
        "# 2. Bag of Words(BoW)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6c33a54",
      "metadata": {
        "id": "d6c33a54"
      },
      "source": [
        "Bag of Words (BoW) is a text representation technique in Natural Language Processing (NLP) where a text (such as a sentence or document) is represented as an unordered collection of its words, disregarding grammar and word order, but keeping multiplicity (frequency).\n",
        "\n",
        "BoW builds a vocabulary of all the unique words from the entire corpus (dataset) and represents each document by counting how many times each word appears in it.\n",
        "\n",
        "    It ignores the position of the words.\n",
        "\n",
        "    The result is a sparse vector where each element represents the count of a word from the vocabulary.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f05887c0",
      "metadata": {
        "id": "f05887c0"
      },
      "source": [
        "`Doc 1`: \"I love NLP\"\n",
        "\n",
        "`Doc 2`: \"I love machine learning\"\n",
        "\n",
        "`Vocabulary`: ['I', 'love', 'NLP', 'machine', 'learning']\n",
        "\n",
        "`BoW representation`:\n",
        "\n",
        "Document:\tI\tlove\tNLP\tmachine\tlearning\n",
        "\n",
        "Doc 1\t:   1\t1\t1\t0\t0\n",
        "\n",
        "Doc 2\t:   1\t1\t0\t1\t1\n",
        "\n",
        "Each row is a vector representation of a document."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e134062",
      "metadata": {
        "id": "0e134062"
      },
      "source": [
        "| Advantage                                  | Description                                         |\n",
        "| ------------------------------------------ | --------------------------------------------------- |\n",
        "|  **Simple and intuitive**                 | Easy to implement and understand.                   |\n",
        "|  **Captures word frequency**             | Gives information about how often words appear.     |\n",
        "|  **Works well with classical ML models** | Such as Naive Bayes, SVM, Logistic Regression, etc. |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67b6bce7",
      "metadata": {
        "id": "67b6bce7"
      },
      "source": [
        "| Disadvantage                            | Description                                                        |\n",
        "| --------------------------------------- | ------------------------------------------------------------------ |\n",
        "|  **Ignores semantics and word order** | Cannot detect meaning, synonyms, or context.                       |\n",
        "|  **Large and sparse vectors**         | High-dimensional space if vocabulary is large.                     |\n",
        "|  **No understanding of meaning**      | \"I love dogs\" and \"Dogs love me\" are treated as identical in BoW.  |\n",
        "|  **Weights all words equally**        | Common words like \"the\" may dominate unless stopwords are removed. |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e47aee73",
      "metadata": {
        "id": "e47aee73",
        "outputId": "72236e70-df45-4645-a378-bb2ab716b982"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary: ['learning' 'love' 'machine' 'nlp']\n",
            "[[0 1 0 1]\n",
            " [1 1 1 0]]\n"
          ]
        }
      ],
      "source": [
        "# Example in Python using CountVectorizer from scikit-learn\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\n",
        "    \"I love NLP\",\n",
        "    \"I love machine learning\"\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print('Vocabulary:', vectorizer.get_feature_names_out())\n",
        "print(X.toarray())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "699aa530",
      "metadata": {
        "id": "699aa530"
      },
      "source": [
        "# 3. N-grams"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "747d15d0",
      "metadata": {
        "id": "747d15d0"
      },
      "source": [
        "N-grams are contiguous sequences of n items (usually words or characters) from a given text.\n",
        "In NLP, N-grams are most often used to capture context and word order in token sequences.\n",
        "\n",
        "    Unigram = 1 word\n",
        "\n",
        "    Bigram = 2 consecutive words\n",
        "\n",
        "    Trigram = 3 consecutive words\n",
        "\n",
        "    … and so on.\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "585aae96",
      "metadata": {
        "id": "585aae96"
      },
      "source": [
        "The N-gram model is a simple and widely used method for text representation and language modeling.\n",
        "\n",
        "Unlike Bag of Words (which ignores word order), N-grams help capture phrase structure and local context by looking at adjacent words.\n",
        "\n",
        "    For example, the sentence:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "620d70c2",
      "metadata": {
        "id": "620d70c2"
      },
      "source": [
        "\"I love machine learning\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44287582",
      "metadata": {
        "id": "44287582"
      },
      "source": [
        "    Unigrams:\n",
        "    ['I', 'love', 'machine', 'learning']\n",
        "\n",
        "    Bigrams (n=2):\n",
        "    ['I love', 'love machine', 'machine learning']\n",
        "\n",
        "    Trigrams (n=3):\n",
        "    ['I love machine', 'love machine learning']\n",
        "\n",
        "These are often used to build statistical language models or as features in classification tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c990d3a",
      "metadata": {
        "id": "5c990d3a",
        "outputId": "e49841b0-a47c-4f69-912e-306745f4d64d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['love machine' 'machine learning']\n",
            "[[1 1]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\"I love machine learning\"]\n",
        "\n",
        "# Extract Bigrams (2-grams)\n",
        "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(X.toarray())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf0f118e",
      "metadata": {
        "id": "bf0f118e"
      },
      "source": [
        "| Advantage                           | Description                                                               |\n",
        "| ----------------------------------- | ------------------------------------------------------------------------- |\n",
        "|  **Captures context and order**    | Retains the local structure of language (e.g., “not good” vs “good”).     |\n",
        "|  **Improves text classification** | Especially effective for tasks like sentiment analysis or spam detection. |\n",
        "|  **Useful in language modeling**  | Helps in predicting the next word in a sequence.                          |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dabbec92",
      "metadata": {
        "id": "dabbec92"
      },
      "source": [
        "| Disadvantage                | Description                                                          |\n",
        "| --------------------------- | -------------------------------------------------------------------- |\n",
        "|  **Data sparsity**        | As `n` increases, combinations become sparse and hard to learn from. |\n",
        "|  **High dimensionality**  | Large N-grams result in huge feature spaces, requiring more memory.  |\n",
        "|  **Limited context**      | Still fails to capture long-term dependencies and meaning.           |\n",
        "|  **Vocabulary explosion** | Many similar phrases are treated as completely separate features.    |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fa3448f",
      "metadata": {
        "id": "1fa3448f"
      },
      "source": [
        "# 4. TF-IDF (Term Frequecny Inverse Document Frequency)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8ed5736",
      "metadata": {
        "id": "b8ed5736"
      },
      "source": [
        "TF-IDF is a numerical statistic that reflects how important a word is to a document in a collection (corpus). It is widely used in information retrieval and text mining.\n",
        "\n",
        "It balances two ideas:\n",
        "\n",
        "   - TF (Term Frequency): How often a term appears in a document.\n",
        "\n",
        "   - IDF (Inverse Document Frequency): How rare the term is across all documents."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e94bc4b",
      "metadata": {
        "id": "5e94bc4b"
      },
      "source": [
        "#### Formula:\n",
        "\n",
        "TF-IDF = TF(t, d) × IDF(t)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbd1d868",
      "metadata": {
        "id": "bbd1d868"
      },
      "source": [
        "Where:\n",
        "\n",
        "   - t = term (word)\n",
        "\n",
        "   - d = document\n",
        "\n",
        "   - TF(t, d) = (Number of times term t appears in document d) / (Total terms in d)\n",
        "\n",
        "   - IDF(t) = log(N / df(t)), where:\n",
        "\n",
        "       - N = total number of documents\n",
        "\n",
        "       - df(t) = number of documents containing term t"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f3dbcf6",
      "metadata": {
        "id": "4f3dbcf6"
      },
      "source": [
        "#### Explanation\n",
        "\n",
        "The idea is to:\n",
        "\n",
        "  -  Emphasize important words in a document (high TF).\n",
        "\n",
        "  -  Downweight common words like “the”, “is”, “and” (low IDF).\n",
        "\n",
        "So, rare but frequent words in a document get the highest scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87cf635c",
      "metadata": {
        "id": "87cf635c"
      },
      "outputs": [],
      "source": [
        "docs = [\n",
        "    \"I love machine learning\",\n",
        "    \"I love deep learning\",\n",
        "    \"machine learning is amazing\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94c82efe",
      "metadata": {
        "id": "94c82efe",
        "outputId": "f0e580f1-b004-4fda-a24b-0faab1e665ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['amazing' 'deep' 'is' 'learning' 'love' 'machine']\n",
            "[[0.         0.         0.         0.48133417 0.61980538 0.61980538]\n",
            " [0.         0.72033345 0.         0.42544054 0.54783215 0.        ]\n",
            " [0.5844829  0.         0.5844829  0.34520502 0.         0.44451431]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(docs)\n",
        "\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(X.toarray())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "147315a1",
      "metadata": {
        "id": "147315a1"
      },
      "source": [
        "| Pros                             | Description                                          |\n",
        "| -------------------------------- | ---------------------------------------------------- |\n",
        "|  **Captures term importance**   | Highlights keywords unique to a document.            |\n",
        "|  **Reduces common word impact** | Common across documents = less weight.               |\n",
        "|  **Efficient & Scalable**       | Simple and fast to compute even on large corpora.    |\n",
        "|  **Better than BoW**            | Avoids overvaluing frequent but uninformative words. |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5633df26",
      "metadata": {
        "id": "5633df26"
      },
      "source": [
        "| Cons                        | Description                                  |\n",
        "| --------------------------- | -------------------------------------------- |\n",
        "|  **No semantics**          | “good” and “excellent” treated as unrelated. |\n",
        "|  **Sparse representation** | Large vocabularies → many zero entries.      |\n",
        "|  **Fixed vocabulary**      | Can't handle unseen words during inference.  |\n",
        "|  **Ignores word order**    | Loses sequence information like N-grams.     |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cc737cb",
      "metadata": {
        "id": "8cc737cb"
      },
      "source": [
        "# 5. Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10ced336",
      "metadata": {
        "id": "10ced336"
      },
      "source": [
        "Word embedding is a term used for hte representation of words for text analysis, typically in the form of a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning.\n",
        "\n",
        "Word2Vec is a word embedding technique that transforms words into dense vector representations based on their context in a corpus. It captures semantic meaning, so similar words have similar vectors.\n",
        "\n",
        "  -  Developed by Tomas Mikolov et al. at Google in 2013."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "990c7548",
      "metadata": {
        "id": "990c7548"
      },
      "source": [
        "### Explanation:\n",
        "\n",
        "Word2Vec learns vector representations of words.\n",
        "⚙️ It has two main architectures:\n",
        "\n",
        "  1.  CBOW (Continuous Bag of Words)\n",
        "\n",
        "      -  Predicts the target word from the surrounding context words.\n",
        "\n",
        "      -  Example: \"I ___ NLP\" → predict \"love\".\n",
        "\n",
        "  2.  Skip-Gram\n",
        "\n",
        "      -  Predicts context words from the target word.\n",
        "\n",
        "      -  Example: \"NLP\" → predict \"I\", \"love\", etc.\n",
        "\n",
        "Both use a shallow neural network (one hidden layer) to learn these relationships."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb912706",
      "metadata": {
        "id": "eb912706"
      },
      "source": [
        "| Pros                             | Description                                                              |\n",
        "| -------------------------------- | ------------------------------------------------------------------------ |\n",
        "|  **Semantic meaning**           | Similar words have similar vectors.                                      |\n",
        "|  **Efficient & scalable**       | Trains fast on large corpora.                                            |\n",
        "|  **Handles large vocabularies** | Learns compact, meaningful representations.                              |\n",
        "|  **Works well with analogies**  | “king - man + woman ≈ queen”.                                            |\n",
        "|  **Improves downstream tasks**  | Better input features for NLP tasks (e.g., classification, translation). |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6173038c",
      "metadata": {
        "id": "6173038c"
      },
      "source": [
        "| Cons                         | Description                                                                          |\n",
        "| ---------------------------- | ------------------------------------------------------------------------------------ |\n",
        "|  **Requires large corpora** | Needs a lot of data to perform well.                                                 |\n",
        "|  **Context-independent**    | Same vector for a word regardless of sentence (e.g., \"bank\" in \"river\" vs. \"money\"). |\n",
        "|  **No OOV handling**        | Cannot represent words not seen during training.                                     |\n",
        "|  **Ignores morphology**     | “run”, “running”, and “ran” are unrelated unless explicitly trained.                 |\n",
        "|  **Static embeddings**      | One word = one meaning.                                                              |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "pRdMlUtqQ2sU",
      "metadata": {
        "id": "pRdMlUtqQ2sU"
      },
      "outputs": [],
      "source": [
        "import gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "q9ybdR4xK5fI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9ybdR4xK5fI",
        "outputId": "68aead51-459e-462a-8981-fecc11268be5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ],
      "source": [
        "# Download and load the model\n",
        "import gensim.downloader as api\n",
        "\n",
        "model = api.load(\"word2vec-google-news-300\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "QWMQ-iVjKGki",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWMQ-iVjKGki",
        "outputId": "8b68b0c8-f377-496b-968b-9559429ecec9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(300,)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model['cricket'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "xzojUiaEKGh4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzojUiaEKGh4",
        "outputId": "0e860f73-7f6e-47d5-deb2-5ffcffcb272e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('woman', 0.7664012908935547),\n",
              " ('boy', 0.6824871301651001),\n",
              " ('teenager', 0.6586930155754089),\n",
              " ('teenage_girl', 0.6147903203964233),\n",
              " ('girl', 0.5921714305877686),\n",
              " ('suspected_purse_snatcher', 0.571636438369751),\n",
              " ('robber', 0.5585119128227234),\n",
              " ('Robbery_suspect', 0.5584409832954407),\n",
              " ('teen_ager', 0.5549196600914001),\n",
              " ('men', 0.5489763021469116)]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.most_similar('man')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "YEklkQumKGfE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEklkQumKGfE",
        "outputId": "bacd9d9e-32b8-468f-f9cc-01020a44e3c1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('cricketing', 0.8372225761413574),\n",
              " ('cricketers', 0.8165745735168457),\n",
              " ('Test_cricket', 0.8094819188117981),\n",
              " ('Twenty##_cricket', 0.8068488240242004),\n",
              " ('Twenty##', 0.7624265551567078),\n",
              " ('Cricket', 0.75413978099823),\n",
              " ('cricketer', 0.7372578382492065),\n",
              " ('twenty##', 0.7316356897354126),\n",
              " ('T##_cricket', 0.7304614186286926),\n",
              " ('West_Indies_cricket', 0.6987985968589783)]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.most_similar('cricket')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "Ad1v3FflKGcC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ad1v3FflKGcC",
        "outputId": "10469822-f0c5-4ff9-9054-52856ae3c036"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Facebook', 0.7563533186912537),\n",
              " ('FaceBook', 0.7076998949050903),\n",
              " ('twitter', 0.6988552212715149),\n",
              " ('myspace', 0.6941817998886108),\n",
              " ('Twitter', 0.664244532585144),\n",
              " ('twitter_facebook', 0.6572229862213135),\n",
              " ('Facebook.com', 0.6529868245124817),\n",
              " ('myspace_facebook', 0.6370643973350525),\n",
              " ('facebook_twitter', 0.6367618441581726),\n",
              " ('linkedin', 0.6356592774391174)]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.most_similar('facebook')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "bIRs-7N4KGZQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIRs-7N4KGZQ",
        "outputId": "2ca90efd-9a5c-4865-e06c-7dd9894ad02d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.76640123"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.similarity('man','woman')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "VAynOV-SKGWl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAynOV-SKGWl",
        "outputId": "55586fd2-3e6d-4a2c-8cfe-5a454bfe3f9f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-0.032995153"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.similarity('man','PHP')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "givVA5uFKGRB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "givVA5uFKGRB",
        "outputId": "abe2cb6f-4939-4c20-9b77-0f5e228aaabd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'monkey'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.doesnt_match(['PHP','java','monkey'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "GRqBHR2LKW63",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRqBHR2LKW63",
        "outputId": "c8b84c9f-337b-4f28-ccbc-15023e71a68d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('king', 0.8449392318725586),\n",
              " ('queen', 0.7300517559051514),\n",
              " ('monarch', 0.645466148853302),\n",
              " ('princess', 0.6156251430511475),\n",
              " ('crown_prince', 0.5818676352500916),\n",
              " ('prince', 0.5777117609977722),\n",
              " ('kings', 0.5613663792610168),\n",
              " ('sultan', 0.5376775860786438),\n",
              " ('Queen_Consort', 0.5344247817993164),\n",
              " ('queens', 0.5289887189865112)]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vec = model['king'] - model['man'] + model['woman']\n",
        "model.most_similar([vec])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "hl8ZxRvMKW30",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hl8ZxRvMKW30",
        "outputId": "f14d8d99-53ca-43f2-cdc1-ea954395b0e2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('forex', 0.7094285488128662),\n",
              " ('coin', 0.5928510427474976),\n",
              " ('Forex', 0.518968939781189),\n",
              " ('coins', 0.5181783437728882),\n",
              " ('Currency', 0.5116606950759888),\n",
              " ('curency', 0.4965347647666931),\n",
              " ('currency', 0.4918244779109955),\n",
              " ('FOREX', 0.48118162155151367),\n",
              " ('FXCM_Micro', 0.4436749219894409),\n",
              " ('interbank_forex', 0.43405434489250183)]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vec = model['coin'] - model ['crypto'] + model['forex']\n",
        "model.most_similar([vec])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "oscwqAMcPCzT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oscwqAMcPCzT",
        "outputId": "60e11641-571c-499d-f0e9-f0a47a6d0ae5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('crypto', 0.6437585353851318),\n",
              " ('coin', 0.49621814489364624),\n",
              " ('proto', 0.3795802891254425),\n",
              " ('Crypto', 0.3647187054157257),\n",
              " ('swastika_emblazoned', 0.34711766242980957),\n",
              " ('coinage', 0.34510746598243713),\n",
              " ('Stakes_Albarado', 0.33207038044929504),\n",
              " ('medallion', 0.33010321855545044),\n",
              " ('eponym', 0.3290451467037201),\n",
              " ('coins', 0.32545778155326843)]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vec = model['coin'] - model ['forex'] + model['crypto']\n",
        "model.most_similar([vec])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 13.057316,
      "end_time": "2025-06-30T07:42:48.794141",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-06-30T07:42:35.736825",
      "version": "2.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
